{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe222966-47d3-4798-b655-92c2ac255653",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/expressions.py:1134: UserWarning: WARN WindowExpression: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nPlik: SRR16356246_1.fastq - pierwsze 5 rekordów po czyszczeniu nagłówka:\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/expressions.py:1134: UserWarning: WARN WindowExpression: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, row_number, monotonically_increasing_id, when, first, regexp_replace, split\n",
    "from pyspark.sql.window import Window\n",
    "import os\n",
    "\n",
    "# 1. Tworzymy sesję Spark\n",
    "spark = SparkSession.builder.appName(\"FASTQ_Clean_Header\").getOrCreate()\n",
    "\n",
    "# 2. Ścieżka do katalogu z FASTQ\n",
    "fastq_dir = \"/Volumes/databrics_2/default/fastq\"\n",
    "\n",
    "# 3. Lista plików FASTQ\n",
    "files = [\"SRR16356246_1.fastq\", \"SRR16356246_2.fastq\"]\n",
    "\n",
    "for file_name in files:\n",
    "    file_path = os.path.join(fastq_dir, file_name)\n",
    "    \n",
    "    # 4. Wczytanie pliku do DataFrame Spark\n",
    "    lines_df = spark.read.text(file_path)\n",
    "    \n",
    "    # 5. Dodanie numeru linii\n",
    "    window = Window.orderBy(monotonically_increasing_id())\n",
    "    lines_with_id = lines_df.withColumn(\"line_number\", row_number().over(window) - 1)\n",
    "    \n",
    "    # 6. Określenie typu linii FASTQ\n",
    "    lines_typed = lines_with_id.withColumn(\n",
    "        \"line_type\",\n",
    "        when(col(\"line_number\") % 4 == 0, \"header\")\n",
    "        .when(col(\"line_number\") % 4 == 1, \"sequence\")\n",
    "        .when(col(\"line_number\") % 4 == 2, \"separator\")\n",
    "        .when(col(\"line_number\") % 4 == 3, \"quality\")\n",
    "    )\n",
    "    \n",
    "    # 7. Utworzenie record_id\n",
    "    lines_with_record = lines_typed.withColumn(\n",
    "        \"record_id\",\n",
    "        (col(\"line_number\") / 4).cast(\"integer\")\n",
    "    )\n",
    "    \n",
    "    # 8. Pivot: format szeroki (1 wiersz na odczyt)\n",
    "    fastq_wide = lines_with_record.groupBy(\"record_id\").pivot(\"line_type\").agg(\n",
    "        first(\"value\").alias(\"value\")\n",
    "    )\n",
    "\n",
    "    # 9. Kolumny pivotowane mają teraz aliasy 'value' - nadajemy właściwe nazwy\n",
    "    fastq_wide = fastq_wide.select(\n",
    "        col(\"record_id\"),\n",
    "        col(\"header\").alias(\"header\") if \"header\" in fastq_wide.columns else col(\"value\").alias(\"header\"),\n",
    "        col(\"sequence\").alias(\"sequence\") if \"sequence\" in fastq_wide.columns else col(\"value\").alias(\"sequence\"),\n",
    "        col(\"quality\").alias(\"quality\") if \"quality\" in fastq_wide.columns else col(\"value\").alias(\"quality\")\n",
    "    )\n",
    "    \n",
    "    # 10. Czyszczenie nagłówka, wyodrębnienie read_id\n",
    "    fastq_clean = fastq_wide.withColumn(\n",
    "        \"read_id\",\n",
    "        split(regexp_replace(col(\"header\"), \"^@\", \"\"), \" \")[0]\n",
    "    ).select(\"record_id\", \"read_id\", \"sequence\", \"quality\")\n",
    "    \n",
    "    # 11. Wyświetlenie pierwszych 5 rekordów\n",
    "    print(f\"\\nPlik: {file_name} - pierwsze 5 rekordów po czyszczeniu nagłówka:\")\n",
    "    fastq_clean.show(5, truncate=False)\n",
    "    \n",
    "    # 12. Liczba odczytów w pliku\n",
    "    total_reads = fastq_clean.count()\n",
    "    print(f\"Liczba odczytów w pliku {file_name}: {total_reads}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Klastry_8.6",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}