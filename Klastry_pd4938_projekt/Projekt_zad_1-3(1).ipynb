{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "335f24ed-6e0d-408b-a3ee-12f9ccbc4bf0",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 1"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/expressions.py:1134: UserWarning: WARN WindowExpression: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+----+-------+\n|header|sequence|plus|quality|\n+------+--------+----+-------+\n+------+--------+----+-------+\n\nZadanie 1 - liczba niespójnych rekordów: 0\nZadanie 2 - liczba odczytów o bardzo niskiej jakości (znak '#'): 0\nZadanie 3 - liczba grup połączenia długości i niskiej jakości: 0\nZadanie 3 - liczba unikalnych długości sekwencji: 0\n+---------------+----------+-----+\n|has_low_quality|seq_length|count|\n+---------------+----------+-----+\n+---------------+----------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# =====================================================\n",
    "# KOMPLETNY PROJEKT: ZADANIE 1, 2 i 3\n",
    "# =====================================================\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, row_number, floor, collect_list, regexp_extract, length\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# ==============================\n",
    "# 1. SparkSession\n",
    "# ==============================\n",
    "spark = SparkSession.builder.appName(\"FASTQ_Complex_Analysis\").getOrCreate()\n",
    "\n",
    "# ==============================\n",
    "# 2. Ścieżki do FASTQ\n",
    "# ==============================\n",
    "fastq_dir = \"/Volumes/databrics_2/default/fastq\"\n",
    "files = [\"SRR16356246_1.fastq\", \"SRR16356246_2.fastq\"]\n",
    "paths = [f\"{fastq_dir}/{file}\" for file in files]\n",
    "\n",
    "# ==============================\n",
    "# 3. Wczytanie plików jako tekst\n",
    "# ==============================\n",
    "raw_df = spark.read.text(paths)\n",
    "\n",
    "# ==============================\n",
    "# 4. Numerowanie wierszy\n",
    "# ==============================\n",
    "window = Window.orderBy(\"value\")\n",
    "df_indexed = raw_df.withColumn(\"row_num\", row_number().over(window))\n",
    "\n",
    "# ==============================\n",
    "# 5. Grupowanie co 4 linie (FASTQ)\n",
    "# ==============================\n",
    "df_indexed = df_indexed.withColumn(\"group_id\", floor((col(\"row_num\") - 1) / 4))\n",
    "\n",
    "fastq_grouped = df_indexed.groupBy(\"group_id\") \\\n",
    "    .agg(collect_list(\"value\").alias(\"lines\"))\n",
    "\n",
    "# ==============================\n",
    "# 6. Mapowanie na kolumny FASTQ\n",
    "# ==============================\n",
    "fastq_df = fastq_grouped.select(\n",
    "    col(\"lines\").getItem(0).alias(\"header\"),\n",
    "    col(\"lines\").getItem(1).alias(\"sequence\"),\n",
    "    col(\"lines\").getItem(2).alias(\"plus\"),\n",
    "    col(\"lines\").getItem(3).alias(\"quality\")\n",
    ")\n",
    "\n",
    "fastq_df.show(5, truncate=False)\n",
    "\n",
    "# ==============================\n",
    "# ZADANIE 1: WALIDACJA DŁUGOŚCI\n",
    "# ==============================\n",
    "df_with_lengths = fastq_df.withColumn(\n",
    "    \"declared_length\",\n",
    "    regexp_extract(col(\"header\"), r\"length=(\\d+)\", 1).cast(\"int\")\n",
    ").withColumn(\n",
    "    \"actual_length\",\n",
    "    length(col(\"sequence\"))\n",
    ")\n",
    "\n",
    "invalid_records = df_with_lengths.filter(\n",
    "    col(\"declared_length\") != col(\"actual_length\")\n",
    ")\n",
    "\n",
    "invalid_count = invalid_records.count()\n",
    "print(\"Zadanie 1 - liczba niespójnych rekordów:\", invalid_count)\n",
    "\n",
    "# ==============================\n",
    "# ZADANIE 2: WSTĘPNA ANALIZA JAKOŚCI\n",
    "# ==============================\n",
    "df_quality = fastq_df.withColumn(\n",
    "    \"low_quality\",\n",
    "    col(\"quality\").contains(\"#\")\n",
    ")\n",
    "\n",
    "low_quality_reads = df_quality.filter(col(\"low_quality\") == True)\n",
    "\n",
    "low_quality_count = low_quality_reads.count()\n",
    "print(\"Zadanie 2 - liczba odczytów o bardzo niskiej jakości (znak '#'):\", low_quality_count)\n",
    "\n",
    "# ==============================\n",
    "# ZADANIE 3: KOMPLEKSOWA ANALIZA + CACHE\n",
    "# ==============================\n",
    "complex_df = fastq_df.withColumn(\n",
    "    \"has_low_quality\",\n",
    "    col(\"quality\").contains(\"#\")\n",
    ").withColumn(\n",
    "    \"seq_length\",\n",
    "    length(col(\"sequence\"))\n",
    ")\n",
    "\n",
    "complex_grouped = complex_df.groupBy(\n",
    "    \"has_low_quality\",\n",
    "    \"seq_length\"\n",
    ").count().orderBy(\"has_low_quality\", \"seq_length\")\n",
    "\n",
    "# Cache wyniku - removed .cache() as it's not supported on serverless\n",
    "complex_grouped_cached = complex_grouped\n",
    "\n",
    "# Akcja 1: count\n",
    "total_groups = complex_grouped_cached.count()\n",
    "print(\"Zadanie 3 - liczba grup połączenia długości i niskiej jakości:\", total_groups)\n",
    "\n",
    "# Akcja 2: count unikalnych długości\n",
    "distinct_lengths = complex_grouped_cached.select(\"seq_length\").distinct().count()\n",
    "print(\"Zadanie 3 - liczba unikalnych długości sekwencji:\", distinct_lengths)\n",
    "\n",
    "# Akcja 3: pokazanie pierwszych 10 wierszy\n",
    "complex_grouped_cached.show(10, truncate=False)\n",
    "\n",
    "# ==============================\n",
    "# KOMENTARZE ANALITYCZNE\n",
    "# ==============================\n",
    "\n",
    "# Zadanie 1:\n",
    "# - Operacje regexp_extract + length → droższe\n",
    "# - Liczba Jobów = 1 (akacja .count())\n",
    "# - Stage'y: narrow transformations\n",
    "# - Taski = liczba partycji\n",
    "\n",
    "# Zadanie 2:\n",
    "# - Operacja .contains(\"#\") → szybka\n",
    "# - Stage'y mogą być oznaczone jako \"skipped\", jeśli fastq_df było cache'owane\n",
    "# - Locality Level tasków = PROCESS_LOCAL\n",
    "# - Liczba Records/task ≈ liczba rekordów w partycji\n",
    "\n",
    "# Zadanie 3:\n",
    "# - groupBy + count → shuffle → Stage 1: shuffle write, Stage 2: shuffle read\n",
    "# - Cache → kolejne akcje (distinct, show) nie liczą od zera, Stage'y mogą być \"skipped\"\n",
    "# - Efektywność cache: pierwszy Job pełny czas, kolejne Joby bardzo szybkie\n",
    "# - Locality Level: PROCESS_LOCAL, jeśli niektóre taski NODE_LOCAL → dane w pamięci innego executor'a"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Projekt_zad_1-3",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}